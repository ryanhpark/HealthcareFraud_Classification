{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.linear_model import \\\n",
    "    LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import \\\n",
    "    RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from imblearn import over_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = load('./data/Providers_Third.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = providers.drop('PotentialFraud', axis=1)\n",
    "y = providers.PotentialFraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 70/30 split gives roughly the same baseline model results\n",
    "# # as 80/20 and 90/10, but saves grid_search time\n",
    "\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale only the training data to avoid data leakage\n",
    "# # MinMax performed better or about the same as StandardScaler,\n",
    "# # RobustScaler, and Normalize on most models\n",
    "\n",
    "scaler = pp.MinMaxScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample for K-nearest Neighbors\n",
    "\n",
    "oversample = over_sampling.SMOTE(random_state=0)\n",
    "X_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stratify folds so that classes always have the same sample ratio\n",
    "# # n_splits=10 to maximize testing for this small dataset\n",
    "\n",
    "skfold = ms.StratifiedKFold(n_splits=5, random_state=0, shuffle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results(model,\n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "        train_recall = recall_score(y_train, model.predict(X_train))\n",
    "        test_recall = recall_score(y_test, model.predict(X_test))\n",
    "        \n",
    "        print('Model details:', model, '\\n')\n",
    "        print('Train Set Recall Score:',\n",
    "              f'{round(train_recall, 4) * 100}%')\n",
    "        print('Test Set Recall Score:',\n",
    "              f'{round(test_recall, 4) * 100}%')\n",
    "        print('\\nTrain Set Confusion Matrix:\\n',\n",
    "              confusion_matrix(y_train, model.predict(X_train)))\n",
    "        print('Test Set Confusion Matrix:\\n',\n",
    "              confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: LogisticRegressionCV(class_weight='balanced',\n",
      "                     cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
      "                     n_jobs=-1, random_state=0, scoring='recall', verbose=1) \n",
      "\n",
      "Train Set Recall Score: 87.29%\n",
      "Test Set Recall Score: 86.18%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2256 1177]\n",
      " [  45  309]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1003  468]\n",
      " [  21  131]]\n"
     ]
    }
   ],
   "source": [
    "# # baseline model\n",
    "\n",
    "# logRegCV = \\\n",
    "#     LogisticRegressionCV(class_weight='balanced',\n",
    "#                          cv=skfold, scoring='recall',\n",
    "#                          random_state=0, n_jobs=(-1), verbose=1\n",
    "#                         ).fit(X_train, y_train)\n",
    "\n",
    "# dump(logRegCV, './data/Iteration_3/logRegCV.pkl')\n",
    "\n",
    "logRegCV = load('./data/Iteration_3/logRegCV.pkl')\n",
    "model_results(logRegCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: LogisticRegressionCV(class_weight='balanced',\n",
      "                     cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
      "                     n_jobs=-1, penalty='l1', random_state=0, scoring='recall',\n",
      "                     solver='liblinear', verbose=1) \n",
      "\n",
      "Train Set Recall Score: 90.11%\n",
      "Test Set Recall Score: 89.47%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2891  542]\n",
      " [  35  319]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1251  220]\n",
      " [  16  136]]\n"
     ]
    }
   ],
   "source": [
    "# # baseline model\n",
    "# # L1 penalty for feature selection, liblinear solver faster than saga\n",
    "\n",
    "# logRegCVL1 = \\\n",
    "#     LogisticRegressionCV(penalty='l1', solver='liblinear', cv=skfold,\n",
    "#                          class_weight='balanced', scoring='recall',\n",
    "#                          random_state=0, n_jobs=(-1), verbose=1\n",
    "#                         ).fit(X_train, y_train)\n",
    "\n",
    "# dump(logRegCVL1, './data/Iteration_3/logRegCVL1.pkl')\n",
    "\n",
    "logRegCVL1 = load('./data/Iteration_3/logRegCVL1.pkl')\n",
    "model_results(logRegCVL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: LogisticRegression(C=0.31622776601683794, class_weight='balanced', max_iter=150,\n",
      "                   penalty='l1', random_state=0, solver='liblinear') \n",
      "\n",
      "Train Set Recall Score: 90.4%\n",
      "Test Set Recall Score: 90.13%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2879  554]\n",
      " [  34  320]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1248  223]\n",
      " [  15  137]]\n",
      "\n",
      "About 1% worse than Iteration 1\n"
     ]
    }
   ],
   "source": [
    "# # grid search (fine-tuned)\n",
    "\n",
    "# logRegModel = \\\n",
    "#     LogisticRegression(penalty='l1', solver='liblinear', max_iter=150,\n",
    "#                        class_weight='balanced', random_state=0)\n",
    "\n",
    "# params = {'C': np.logspace(-1, -0.25, 100)}\n",
    "\n",
    "# logRegRS = ms.RandomizedSearchCV(logRegModel, param_distributions=params,\n",
    "#                            scoring='recall', cv=skfold, verbose=1,\n",
    "#                            return_train_score=True\n",
    "#                           ).fit(X_train, y_train)\n",
    "\n",
    "# bestLogReg = logRegRS.best_estimator_\n",
    "\n",
    "# dump(bestLogReg, './data/Iteration_3/bestLogReg.pkl')\n",
    "\n",
    "bestLogReg = load('./data/Iteration_3/bestLogReg.pkl')\n",
    "model_results(bestLogReg)\n",
    "print('\\nAbout 1% worse than Iteration 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grid search tuning steps\n",
    "\n",
    "# params = {'C': np.logspace(-2, 2, 50),\n",
    "#           'max_iter': [150, 200]}\n",
    "# Model details: LogisticRegression(C=0.13894954943731375, class_weight='balanced', max_iter=200,\n",
    "#                    penalty='l1', random_state=0, solver='liblinear') \n",
    "\n",
    "# Train Set Recall Score: 88.7%\n",
    "# Test Set Recall Score: 91.45%\n",
    "\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2797  636]\n",
    "#  [  40  314]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1229  242]\n",
    "#  [  13  139]]\n",
    "\n",
    "# params = {'C': np.logspace(-4, 4, 100),\n",
    "#           'max_iter': [150, 250, 500]}\n",
    "# Model details: LogisticRegression(C=1.0974987654930568, class_weight='balanced', max_iter=150,\n",
    "#                    penalty='l1', random_state=0, solver='liblinear') \n",
    "# Train Set Recall Score: 91.24%\n",
    "# Test Set Recall Score: 88.16000000000001%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2930  503]\n",
    "#  [  31  323]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1261  210]\n",
    "#  [  18  134]]\n",
    "\n",
    "# params = {'C': np.logspace(-4, 4, 100),\n",
    "#           'max_iter': [75, 150, 225]}\n",
    "# Model details: LogisticRegression(C=0.1176811952434999, class_weight='balanced', max_iter=150,\n",
    "#                    penalty='l1', random_state=0, solver='liblinear') \n",
    "\n",
    "# Train Set Recall Score: 88.7%\n",
    "# Test Set Recall Score: 92.11%\n",
    "\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2792  641]\n",
    "#  [  40  314]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1219  252]\n",
    "#  [  12  140]]\n",
    "\n",
    "# params = {'C': np.logspace(-1.25, -0.5, 100),\n",
    "#           'max_iter': [150]}\n",
    "# Model details: LogisticRegression(C=0.2610157215682537, class_weight='balanced', max_iter=150,\n",
    "#                    penalty='l1', random_state=0, solver='liblinear') \n",
    "\n",
    "# Train Set Recall Score: 90.11%\n",
    "# Test Set Recall Score: 91.45%\n",
    "\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2866  567]\n",
    "#  [  35  319]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1243  228]\n",
    "#  [  13  139]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OP_Count_UniquePatients</th>\n",
       "      <td>8.904920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ratio_ClaimsPerPatient</th>\n",
       "      <td>8.490204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Count_UniquePatients</th>\n",
       "      <td>4.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perc_Outpatient</th>\n",
       "      <td>4.029324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_AgeRange</th>\n",
       "      <td>2.390386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Mean_ClaimDuration</th>\n",
       "      <td>2.353617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Mean_DailyClaimCost</th>\n",
       "      <td>2.305638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Mean_AdmitDuration</th>\n",
       "      <td>2.049706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Perc_KidneyDisease_Chronic</th>\n",
       "      <td>1.675708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Mean_ClaimCost</th>\n",
       "      <td>1.516525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_AgeRange</th>\n",
       "      <td>1.489172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Mean_PatientsPerAttPhys</th>\n",
       "      <td>0.809174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Count_UniqueState</th>\n",
       "      <td>0.753946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_MultHosp</th>\n",
       "      <td>0.651565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Perc_HeartFailure_Chronic</th>\n",
       "      <td>0.560429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_Depression_Chronic</th>\n",
       "      <td>0.391796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Mean_NoOfMonths_PartBCov</th>\n",
       "      <td>0.378589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_GenderZero</th>\n",
       "      <td>0.365746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Perc_MultHosp</th>\n",
       "      <td>0.343952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_HeartFailure_Chronic</th>\n",
       "      <td>0.326223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Perc_RaceTwo</th>\n",
       "      <td>0.292912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_IschemicHeart_Chronic</th>\n",
       "      <td>0.275338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perc_DualPatientType</th>\n",
       "      <td>0.263482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_Osteoporosis_Chronic</th>\n",
       "      <td>0.171911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Mean_AnnualReimbursementAmt</th>\n",
       "      <td>0.167512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_No_ProcCode</th>\n",
       "      <td>0.150029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_Alzheimers_Chronic</th>\n",
       "      <td>0.147471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Perc_RheumatoidArthritis_Chronic</th>\n",
       "      <td>0.122475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_HasRenalDisease</th>\n",
       "      <td>0.110705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Perc_RaceOne</th>\n",
       "      <td>0.050823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_Perc_Diabetes_Chronic</th>\n",
       "      <td>0.046076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IP_Mean_InsReimbursementRatio</th>\n",
       "      <td>0.020172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Coefficient\n",
       "OP_Count_UniquePatients                 8.904920\n",
       "Ratio_ClaimsPerPatient                  8.490204\n",
       "IP_Count_UniquePatients                 4.987000\n",
       "Perc_Outpatient                         4.029324\n",
       "OP_AgeRange                             2.390386\n",
       "IP_Mean_ClaimDuration                   2.353617\n",
       "IP_Mean_DailyClaimCost                  2.305638\n",
       "IP_Mean_AdmitDuration                   2.049706\n",
       "OP_Perc_KidneyDisease_Chronic           1.675708\n",
       "IP_Mean_ClaimCost                       1.516525\n",
       "IP_AgeRange                             1.489172\n",
       "OP_Mean_PatientsPerAttPhys              0.809174\n",
       "OP_Count_UniqueState                    0.753946\n",
       "IP_Perc_MultHosp                        0.651565\n",
       "OP_Perc_HeartFailure_Chronic            0.560429\n",
       "IP_Perc_Depression_Chronic              0.391796\n",
       "IP_Mean_NoOfMonths_PartBCov             0.378589\n",
       "IP_Perc_GenderZero                      0.365746\n",
       "OP_Perc_MultHosp                        0.343952\n",
       "IP_Perc_HeartFailure_Chronic            0.326223\n",
       "OP_Perc_RaceTwo                         0.292912\n",
       "IP_Perc_IschemicHeart_Chronic           0.275338\n",
       "Perc_DualPatientType                    0.263482\n",
       "IP_Perc_Osteoporosis_Chronic            0.171911\n",
       "OP_Mean_AnnualReimbursementAmt          0.167512\n",
       "IP_Perc_No_ProcCode                     0.150029\n",
       "IP_Perc_Alzheimers_Chronic              0.147471\n",
       "OP_Perc_RheumatoidArthritis_Chronic     0.122475\n",
       "IP_Perc_HasRenalDisease                 0.110705\n",
       "IP_Perc_RaceOne                         0.050823\n",
       "OP_Perc_Diabetes_Chronic                0.046076\n",
       "IP_Mean_InsReimbursementRatio           0.020172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = pd.DataFrame(bestLogReg.coef_.T, index=X.columns\n",
    "                           ).rename(columns = {0:'Coefficient'}\n",
    "                           ).abs().sort_values(by='Coefficient',\n",
    "                                               ascending=False)\n",
    "coefficients[coefficients.Coefficient > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: KNeighborsClassifier() \n",
      "\n",
      "Train Set Recall Score: 100.0%\n",
      "Test Set Recall Score: 90.13%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2677  756]\n",
      " [   0 3433]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1096  375]\n",
      " [  15  137]]\n"
     ]
    }
   ],
   "source": [
    "# # baseline model with SMOTE\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "dump(knn, './data/Iteration_3/knn.pkl')\n",
    "\n",
    "knn = load('./data/Iteration_3/knn.pkl')\n",
    "model_results(knn, X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: KNeighborsClassifier(n_neighbors=77) \n",
      "\n",
      "Train Set Recall Score: 96.74000000000001%\n",
      "Test Set Recall Score: 94.08%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2203 1230]\n",
      " [ 112 3321]]\n",
      "Test Set Confusion Matrix:\n",
      " [[963 508]\n",
      " [  9 143]]\n",
      "\n",
      "About 2% worse than Iteration 1\n"
     ]
    }
   ],
   "source": [
    "# # grid search (fine-tuned)\n",
    "\n",
    "# params = {'n_neighbors': np.arange(75, 150)}\n",
    "\n",
    "# knnGS = \\\n",
    "#     ms.GridSearchCV(knn, param_grid=params,\n",
    "#                           cv=skfold, n_jobs=(-1), verbose = 1,\n",
    "#                           return_train_score = True,\n",
    "#                           scoring = 'recall').fit(X_train_SMOTE,\n",
    "#                                                   y_train_SMOTE)\n",
    "\n",
    "# bestKNN = knnGS.best_estimator_\n",
    "# dump(bestKNN, './data/Iteration_3/bestKNN.pkl')\n",
    "\n",
    "bestKNN = load('./data/Iteration_3/bestKNN.pkl')\n",
    "model_results(bestKNN, X_train_SMOTE, y_train_SMOTE)\n",
    "print('\\nAbout 2% worse than Iteration 1')\n",
    "\n",
    "# params = {'n_neighbors': np.arange(30, 41)}\n",
    "# Model details: KNeighborsClassifier(n_neighbors=31) \n",
    "# Train Set Recall Score: 98.46000000000001%\n",
    "# Test Set Recall Score: 94.74000000000001%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2282 1151]\n",
    "#  [  53 3380]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[983 488]\n",
    "#  [  8 144]]\n",
    "\n",
    "# params = {'n_neighbors': np.arange(100, 150)}\n",
    "# Model details: KNeighborsClassifier(n_neighbors=103)\n",
    "# Train Set Recall Score: 96.3%\n",
    "# Test Set Recall Score: 94.08%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2196 1237]\n",
    "#  [ 127 3306]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[958 513]\n",
    "#  [  9 143]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: GaussianNB() \n",
      "\n",
      "Train Set Recall Score: 81.92%\n",
      "Test Set Recall Score: 80.92%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2855  578]\n",
      " [  64  290]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1233  238]\n",
      " [  29  123]]\n"
     ]
    }
   ],
   "source": [
    "# # baseline model\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "dump(gnb, './data/Iteration_3/gnb.pkl')\n",
    "\n",
    "gnb = load('./data/Iteration_3/gnb.pkl')\n",
    "model_results(gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    7.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: GaussianNB(var_smoothing=0.05000000000000001) \n",
      "\n",
      "Train Set Recall Score: 88.14%\n",
      "Test Set Recall Score: 86.83999999999999%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2371 1062]\n",
      " [  42  312]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1047  424]\n",
      " [  20  132]]\n",
      "\n",
      "Exactly the same as Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 2800 out of 2800 | elapsed:   15.4s finished\n"
     ]
    }
   ],
   "source": [
    "# # grid search (fine-tuned)\n",
    "\n",
    "params = {'var_smoothing': np.arange(0.04, 0.6, .001)}\n",
    "\n",
    "gnbModel = ms.GridSearchCV(gnb, param_grid=params,\n",
    "                             scoring='recall', cv=skfold,\n",
    "                             n_jobs=(-1), verbose=1\n",
    "                            ).fit(X_train, y_train)\n",
    "\n",
    "bestGNB = gnbModel.best_estimator_\n",
    "dump(bestGNB, './data/Iteration_3/bestGNB.pkl')\n",
    "\n",
    "bestGNB = load('./data/Iteration_3/bestGNB.pkl')\n",
    "model_results(bestGNB)\n",
    "print('\\nExactly the same as Iteration 1')\n",
    "\n",
    "# ----------------------------------------------\n",
    "# params = {'var_smoothing': np.arange(0.01, 0.2, .0001)}\n",
    "# Model details: GaussianNB(var_smoothing=0.050999999999999754) \n",
    "# Train Set Recall Score: 88.42%\n",
    "# Test Set Recall Score: 86.83999999999999%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2425 1008]\n",
    "#  [  41  313]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1077  394]\n",
    "#  [  20  132]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: RandomForestClassifier(class_weight='balanced', random_state=0) \n",
      "\n",
      "Train Set Recall Score: 100.0%\n",
      "Test Set Recall Score: 41.449999999999996%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[3433    0]\n",
      " [   0  354]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1457   14]\n",
      " [  89   63]]\n"
     ]
    }
   ],
   "source": [
    "# # baseline model\n",
    "\n",
    "randForest = \\\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=0\n",
    "                          ).fit(X_train, y_train)\n",
    "\n",
    "dump(randForest, './data/Iteration_3/randForest.pkl')\n",
    "\n",
    "randForest = load('./data/Iteration_3/randForest.pkl')\n",
    "model_results(randForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: RandomForestClassifier(class_weight='balanced_subsample', max_depth=3,\n",
      "                       min_samples_leaf=3, min_samples_split=3,\n",
      "                       n_estimators=800, random_state=0) \n",
      "\n",
      "Train Set Recall Score: 91.24%\n",
      "Test Set Recall Score: 91.45%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2844  589]\n",
      " [  31  323]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1235  236]\n",
      " [  13  139]]\n",
      "\n",
      "About 1% better than Iteration 1\n"
     ]
    }
   ],
   "source": [
    "# # grid search (fine-tuned)\n",
    "\n",
    "# randForest = \\\n",
    "#     RandomForestClassifier(class_weight='balanced_subsample', random_state=0\n",
    "#                           ).fit(X_train, y_train)\n",
    "\n",
    "# params = {'max_depth': np.arange(2, 5),\n",
    "#           'min_samples_split': np.arange(3, 7),\n",
    "#           'min_samples_leaf': np.arange(3, 7),\n",
    "#           'n_estimators': [500, 800, 1000]}\n",
    "\n",
    "# randForestGS = \\\n",
    "#     ms.GridSearchCV(randForest,\n",
    "#                           param_grid=params,\n",
    "#                           scoring='recall', cv=skfold,\n",
    "#                           n_jobs=(-1), verbose=1,\n",
    "#                           return_train_score=True\n",
    "#                           ).fit(X_train, y_train)\n",
    "\n",
    "# bestRandForest = randForestGS.best_estimator_\n",
    "# dump(bestRandForest, './data/Iteration_3/bestRandForest.pkl')\n",
    "\n",
    "bestRandForest = load('./data/Iteration_3/bestRandForest.pkl')\n",
    "model_results(bestRandForest)\n",
    "print('\\nAbout 1% better than Iteration 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grid search tuning steps\n",
    "\n",
    "# params = {'max_depth': np.arange(2, 5),\n",
    "#           'min_samples_split': np.arange(3, 7),\n",
    "#           'min_samples_leaf': np.arange(3, 7)}\n",
    "# Model details: RandomForestClassifier(class_weight='balanced',\n",
    "#                    max_depth=3, min_samples_leaf=3,\n",
    "#                    min_samples_split=3, random_state=0) \n",
    "# Train Set Recall Score: 91.81%\n",
    "# Test Set Recall Score: 88.16000000000001%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2905  528]\n",
    "#  [  29  325]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1268  203]\n",
    "#  [  18  134]]\n",
    "\n",
    "# params = {'max_depth': np.arange(2, 5),\n",
    "#           'min_samples_split': np.arange(3, 7),\n",
    "#           'min_samples_leaf': np.arange(3, 7),\n",
    "#           'n_estimators': [300, 335, 375]}\n",
    "# Model details: RandomForestClassifier(class_weight='balanced', max_depth=3, min_samples_leaf=3,\n",
    "#                        min_samples_split=3, n_estimators=335, random_state=0) \n",
    "# Train Set Recall Score: 91.53%\n",
    "# Test Set Recall Score: 88.82%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2902  531]\n",
    "#  [  30  324]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1263  208]\n",
    "#  [  17  135]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: GradientBoostingClassifier(max_features='auto', random_state=0) \n",
      "\n",
      "Train Set Recall Score: 99.15%\n",
      "Test Set Recall Score: 80.25999999999999%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[3222  211]\n",
      " [   3  351]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1348  123]\n",
      " [  30  122]]\n"
     ]
    }
   ],
   "source": [
    "# # baseline model with sample_weight\n",
    "\n",
    "gradBoostModel = \\\n",
    "    GradientBoostingClassifier(max_features='auto', random_state=0)\n",
    "gradBoostModel.fit(X_train, y_train, sample_weight=\n",
    "    compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "\n",
    "dump(gradBoostModel, './data/Iteration_3/gradBoostModel.pkl')\n",
    "\n",
    "gradBoostModel = load('./data/Iteration_3/gradBoostModel.pkl')\n",
    "model_results(gradBoostModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: GradientBoostingClassifier(learning_rate=0.01, max_depth=2, max_features='auto',\n",
      "                           min_samples_leaf=12, min_samples_split=22,\n",
      "                           n_estimators=200, random_state=0) \n",
      "\n",
      "Train Set Recall Score: 91.81%\n",
      "Test Set Recall Score: 91.45%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2931  502]\n",
      " [  29  325]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1278  193]\n",
      " [  13  139]]\n",
      "\n",
      "About 1% worse than Iteration 1\n"
     ]
    }
   ],
   "source": [
    "# # grid search (fine-tuned)\n",
    "\n",
    "# gradBoostModel = \\\n",
    "#     GradientBoostingClassifier(max_features='auto',\n",
    "#                                learning_rate=0.01,\n",
    "#                                min_samples_leaf=12,\n",
    "#                                random_state=0)\n",
    "\n",
    "# gradBoostModel.fit(X_train, y_train, sample_weight=\n",
    "#     compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "\n",
    "# params = {'n_estimators': [200, 500, 750],\n",
    "#           'min_samples_split': [22, 24, 26],\n",
    "#           'max_depth': [1, 2, 3]}\n",
    "\n",
    "# gradBoostRS = ms.RandomizedSearchCV(gradBoostModel,\n",
    "#                                     param_distributions=params,\n",
    "#                                     scoring='recall', cv=skfold,\n",
    "#                                     n_jobs=(-1), verbose=1,\n",
    "#                                     return_train_score=True)\n",
    "# gradBoostRS.fit(X_train, y_train,\n",
    "#     sample_weight=\\\n",
    "#         compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "\n",
    "# bestGradBoost = gradBoostRS.best_estimator_\n",
    "# dump(bestGradBoost, './data/Iteration_3/bestGradBoost.pkl')\n",
    "\n",
    "bestGradBoost = load('./data/Iteration_3/bestGradBoost.pkl')\n",
    "model_results(bestGradBoost)\n",
    "print('\\nAbout 1% worse than Iteration 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grid search tuning steps\n",
    "\n",
    "# gradBoostModel = \\\n",
    "#     GradientBoostingClassifier(max_features='auto',\n",
    "#                                learning_rate=0.001,\n",
    "#                                min_samples_leaf=12,\n",
    "#                                random_state=0)\n",
    "# params = {'n_estimators': [950, 1050, 1200],\n",
    "#           'min_samples_split': [22, 24, 26],\n",
    "#           'max_depth': [1, 2, 3]}\n",
    "# Model details: GradientBoostingClassifier(learning_rate=0.001, max_depth=2,\n",
    "#                            max_features='auto', min_samples_leaf=12,\n",
    "#                            min_samples_split=24, n_estimators=1050,\n",
    "#                            random_state=0) \n",
    "# Train Set Recall Score: 91.53%\n",
    "# Test Set Recall Score: 90.79%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[2895  538]\n",
    "#  [  30  324]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1261  210]\n",
    "#  [  14  138]]\n",
    "\n",
    "# gradBoostModel = \\\n",
    "#     GradientBoostingClassifier(max_features='auto',\n",
    "#                                learning_rate=0.01,\n",
    "#                                random_state=0)\n",
    "# params = {'n_estimators': [950, 1050, 1200],\n",
    "#           'min_samples_split': [22, 24, 26],\n",
    "#           'max_depth': [1, 2, 3]}\n",
    "# Model details: GradientBoostingClassifier(learning_rate=0.01, max_depth=2, max_features='auto',\n",
    "#                            min_samples_split=26, n_estimators=950,\n",
    "#                            random_state=0) \n",
    "# Train Set Recall Score: 95.19999999999999%\n",
    "# Test Set Recall Score: 87.5%\n",
    "# Train Set Confusion Matrix:\n",
    "#  [[3079  354]\n",
    "#  [  17  337]]\n",
    "# Test Set Confusion Matrix:\n",
    "#  [[1321  150]\n",
    "#  [  19  133]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: SVC(class_weight='balanced', random_state=0) \n",
      "\n",
      "Train Set Recall Score: 93.78999999999999%\n",
      "Test Set Recall Score: 91.45%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2770  663]\n",
      " [  22  332]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1204  267]\n",
      " [  13  139]]\n"
     ]
    }
   ],
   "source": [
    "# # baseline model\n",
    "\n",
    "svm = \\\n",
    "    SVC(random_state=0, class_weight='balanced').fit(X_train, y_train)\n",
    "\n",
    "dump(svm, './data/Iteration_3/svm.pkl')\n",
    "\n",
    "svm = load('./data/Iteration_3/svm.pkl')\n",
    "model_results(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details: SVC(C=13.848863713938718, class_weight='balanced', degree=1, gamma='auto',\n",
      "    random_state=0) \n",
      "\n",
      "Train Set Recall Score: 92.66%\n",
      "Test Set Recall Score: 92.75999999999999%\n",
      "\n",
      "Train Set Confusion Matrix:\n",
      " [[2791  642]\n",
      " [  26  328]]\n",
      "Test Set Confusion Matrix:\n",
      " [[1218  253]\n",
      " [  11  141]]\n",
      "\n",
      "Comparable score to Iteration 1 but fewer false classifications\n"
     ]
    }
   ],
   "source": [
    "# # grid search\n",
    "\n",
    "# params = {'C': np.logspace(-3, 2, 100),\n",
    "#           'kernel': ['linear', 'poly', 'rbf'],\n",
    "#           'degree': [1, 5, 12],\n",
    "#           'gamma': ['scale', 'auto']}\n",
    "\n",
    "# svmRS = \\\n",
    "#     ms.RandomizedSearchCV(svm, param_distributions=params,\n",
    "#                           scoring='recall', cv=skfold,\n",
    "#                           n_jobs=(-1), verbose=1,\n",
    "#                           return_train_score=True\n",
    "#                          ).fit(X_train, y_train)\n",
    "\n",
    "# bestSVM = svmRS.best_estimator_\n",
    "# dump(bestSVM, './data/Iteration_3/bestSVM.pkl')\n",
    "\n",
    "bestSVM = load('./data/Iteration_3/bestSVM.pkl')\n",
    "model_results(bestSVM)\n",
    "print('\\nComparable score to Iteration 1 but fewer false classifications')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
